{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertaillet/Miniproject1/blob/master/Deep_Q-Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If using the notebook on [Colab](https://colab.research.google.com/), run this cell to get the correct files downloaded."
      ],
      "metadata": {
        "id": "I54R3y7XD2J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if \"Miniproject1\" not in os.getcwd():\n",
        "  !git clone https://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@github.com/albertaillet/Miniproject1/\n",
        "  %cd Miniproject1/\n",
        "%ls"
      ],
      "metadata": {
        "id": "nwftAcdqziEE",
        "outputId": "655d54ed-f6e7-490a-e0cf-b233ab49d51a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Miniproject1' already exists and is not an empty directory.\n",
            "/content/Miniproject1\n",
            "Deep_Q-Learning.ipynb  performance_measures.py  README.md   tic_plot.py\n",
            "\u001b[0m\u001b[01;34mimages\u001b[0m/                \u001b[01;34m__pycache__\u001b[0m/             \u001b[01;34mruns\u001b[0m/       tic_tac_toe.ipynb\n",
            "MP_TicTocToe.pdf       Q-learning.ipynb         tic_env.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "uxTyoCM_bvu2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "from IPython.display import clear_output\n",
        "from collections import namedtuple, deque\n",
        "from tic_env import TictactoeEnv, OptimalPlayer\n",
        "timezone_cet = timezone(\"CET\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "SP6liwgV2j1a",
        "outputId": "1d399428-43c2-4dad-e2b8-fd4cd0cdb3c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfZLCGEyzfdm"
      },
      "source": [
        "## 3. Deep Q-Learning\n",
        "\n",
        "As our 2nd algorithm, we use Deep Q-Learning (DQN) combined with $\\epsilon$-greedy policy. You can watch again Part 1 of Deep Reinforcement Learning Lecture 1 for an introduction to DQN and Part 1 of Deep Reinforcement Learning Lecture 2 (in particular slide 8) for more details. The idea in DQN is to approximate $Q$-values by a neural network instead of a look-up table as in Tabular Q-learning. For implementation, you can use ideas from the DQN tutorials of [Keras](https://keras.io/examples/rl/deep_q_network_breakout/) and [PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Implementation details\n",
        "State representation: We represent state $s_{t}$ by a $3 \\times 3 \\times 2$ tensor `x_t`. Each element of `x_t` takes a value of 0 or 1. The $3 \\times 3$ matrix `x_t[:,:,0]` shows positions taken by you, and `x_t[:,:,1]` shows positions taken by your opponent. If `x_t[i, j, 0]=x_t[i, j, 1]=0`, then position $(i, j)$ is available.\n",
        "Neural network architecture: We use a fully connected network. State `x_t` is fed to the network at the input layer. We consider 2 hidden layers each with 128 neurons - with ReLu activation functions. The output layer has 9 neurons (for 9 different actions) with linear activation functions. Each neuron at the output layer shows the $Q$-value of the corresponding action at state `x_t`.\n",
        "\n",
        "Unavailable actions: For DQN, we do not constraint actions to only available actions. However, whenever the agent takes an unavailable action, we end the game and give the agent a negative reward of value $r_{\\text {unav }}=-1$.\n",
        "\n",
        "Free parameters: DQN has many hyper parameters. For convenience, we fix the discount factor at $\\gamma=0.99$. We assume a buffer size of $10^{\\prime} 000$ and a batch size of 64 . We update the target network every 500 games. Instead of squared loss, we use the Huber loss (with $\\delta=1$ ) with Adam optimizer (c.f. the DQN tutorials of Keras and PyTorch). You can fine tune the learning rate if needed, but we suggest $5 \\times 10^{-4}$ as a starting point.\n",
        "\n",
        "Other options? There are tens of different ways to make training of deep networks more efficient. Do you feel like trying some and learning more? You are welcome to do so; you just need to explain the main features of your implementation and a brief summary of your reasoning in less than 300 words under the title 'Implementation details' in your report."
      ],
      "metadata": {
        "id": "VgXbm8jzkJnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parameters\n",
        "\n",
        "HIDDEN_SIZE = 128 #@param {type:\"integer\"}\n",
        "GAMMA = 0.99  #@param {type:\"number\"}\n",
        "ALPHA = 5e-4  #@param {type:\"number\"}\n",
        "BUFFER_SIZE = 10000 #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 64 #@param {type:\"integer\"}\n",
        "LOG_EVERY = 250 #@param {type:\"integer\"}\n",
        "UPDATE_EVERY = 500 #@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "wWR32S7jjYqO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size=18, hidden_size=128, output_size=9):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = x.view(-1, 3, 3, 2)\n",
        "        return self.seq(x)"
      ],
      "metadata": {
        "id": "l8AvlQV12UqC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates a class for the replay buffer and uses code from [Replay Memory](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#replay-memory)"
      ],
      "metadata": {
        "id": "QH-hWdA4pVTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'player'))\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.buffer = deque([], maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.buffer.append(Transition(*args))\n",
        "\n",
        "    def get_batch(self, batch_size=None):\n",
        "        if batch_size is None:\n",
        "          batch_size = self.batch_size\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def has_one_batch(self, batch_size=None):\n",
        "        if batch_size is None:\n",
        "          batch_size = self.batch_size\n",
        "        return len(self) >= batch_size"
      ],
      "metadata": {
        "id": "NkZKkba1kg95"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates a function to call to optimize the model and uses code from \n",
        "[Training loop](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training-loop)"
      ],
      "metadata": {
        "id": "3xpSUJg4vJk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p2v = {'X': 1, 'O': -1}"
      ],
      "metadata": {
        "id": "9YGLB98HMwwG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def state_to_tensor(state, player):\n",
        "  if player==-1:\n",
        "    opponent_player = 1\n",
        "  elif player==1:\n",
        "    opponent_player = -1\n",
        "  elif p2v[player]==-1:\n",
        "    player = p2v[player]\n",
        "    opponent_player = 1\n",
        "  elif p2v[player]==1:\n",
        "    player = p2v[player]\n",
        "    opponent_player = -1\n",
        "  else:\n",
        "    raise ValueError(f\"Player should be 1 or -1, player={player}\")\n",
        "\n",
        "  t = np.zeros((3, 3, 2), dtype=np.float32)\n",
        "  t[:, :, 0] = (state == player)\n",
        "  t[:, :, 1] = (state == opponent_player)\n",
        "  return torch.tensor(t, dtype=torch.float32, device=device)"
      ],
      "metadata": {
        "id": "n5PtNeGQKG7g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepEpsilonGreedy:\n",
        "    def __init__(self, epsilon=0, n_actions=9):\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "        self.player = None\n",
        "    \n",
        "    def set_epsilon(self, epsilon):\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def set_player(self, player):\n",
        "        self.player = player\n",
        "    \n",
        "    def act(self, state):\n",
        "      if np.random.random() > self.epsilon:\n",
        "          state = state_to_tensor(state, self.player)\n",
        "          with torch.no_grad():\n",
        "              return Q1(state).max(1).indices.item()\n",
        "      else:\n",
        "          return random.randrange(self.n_actions)"
      ],
      "metadata": {
        "id": "v3Mrwx7_FDkV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model(Q1: nn.Module, \n",
        "                   Q2: nn.Module, \n",
        "                   buffer: ReplayBuffer, \n",
        "                   optimizer: optim.Optimizer):\n",
        "    if not buffer.has_one_batch():\n",
        "        return\n",
        "    transitions = buffer.get_batch()\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    state_batch = torch.stack([state_to_tensor(s, p) for s, p in zip(batch.state, batch.player)])\n",
        "    action_batch = torch.tensor(batch.action, device=device)\n",
        "    reward_batch = torch.tensor(batch.reward, device=device)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    if non_final_mask.sum().item() > 0:\n",
        "      non_final_next_states = torch.stack([state_to_tensor(s, p) for s, p in zip(batch.next_state, batch.player)\n",
        "                                                  if s is not None])\n",
        "    \n",
        "    # See slide 13 Lecture 10\n",
        "\n",
        "    # Compute Q1(s, a)\n",
        "    state_action_values = Q1(state_batch).take(action_batch)\n",
        "\n",
        "    # Compute max_a' Q2(s', a'), it is set to 0 when the state is final\n",
        "    next_state_values = torch.zeros(action_batch.size()[0], device=device)\n",
        "    if non_final_mask.sum().item() > 0:\n",
        "      next_state_values[non_final_mask] =  Q2(non_final_next_states).max(1).values.detach()\n",
        "\n",
        "    # Compute the expected Q values: (r + gamma * max_a' Q2(s', a'))\n",
        "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.HuberLoss(delta=1.0)\n",
        "    loss = criterion(state_action_values, expected_state_action_values)\n",
        "    loss_val = loss.item()\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in Q1.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "    return loss_val"
      ],
      "metadata": {
        "id": "bdgnv1gMmAor"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 11\n",
        "\n",
        "Plot average reward and average training loss for every 250 games during training. Does the loss decrease? Does the agent learn to play Tic Tac Toe?"
      ],
      "metadata": {
        "id": "MVu0twVi1txM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.5\n",
        "epsilon_opt = 0.5\n",
        "\n",
        "env = TictactoeEnv()\n",
        "buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "Q1 = DQN(hidden_size=HIDDEN_SIZE)\n",
        "Q2 = DQN(hidden_size=HIDDEN_SIZE)\n",
        "Q2.load_state_dict(Q1.state_dict())\n",
        "optimizer = optim.Adam(Q1.parameters())\n",
        "policy = DeepEpsilonGreedy(epsilon=epsilon, n_actions=9)\n",
        "date = datetime.now(timezone_cet).strftime(\"%b%d_%H-%M\")\n",
        "writer = SummaryWriter(log_dir=f\"runs/{date} Epsilon: {epsilon}, Epsilon opt: {epsilon_opt}, Batch Size: {BATCH_SIZE}, Buffer Size: {BUFFER_SIZE}\")"
      ],
      "metadata": {
        "id": "W-Kg9Q138InN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_trained_games = 0\n",
        "av_rewards = []\n",
        "av_loss = []"
      ],
      "metadata": {
        "id": "KaSLPKwbt8Ep"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_games = 20000\n",
        "opponent_player = OptimalPlayer(epsilon_opt, player='O')\n",
        "policy.set_player('X')\n",
        "rewards = []\n",
        "losses = []\n",
        "n_unavilable_actions_taken = 0\n",
        "\n",
        "for itr in range(num_games):\n",
        "    env.reset()\n",
        "    state, end, _ = env.observe()\n",
        "\n",
        "    opponent_player.player, policy.player = policy.player, opponent_player.player\n",
        "\n",
        "    if opponent_player.player == 'X':\n",
        "        opponent_move = opponent_player.act(state)\n",
        "        state, end, _ = env.step(opponent_move)\n",
        "\n",
        "    while not end:\n",
        "        move = policy.act(state)\n",
        "        valid_move = env.check_valid(move)\n",
        "        \n",
        "        if valid_move:\n",
        "          next_state, end, _ = env.step(move)\n",
        "\n",
        "        if valid_move and (not end) and (env.current_player == opponent_player.player):\n",
        "          opponent_move = opponent_player.act(next_state)\n",
        "          next_state, end, _ = env.step(opponent_move)\n",
        "          reward = env.reward(player=policy.player)\n",
        "          buffer.push(state, move, next_state, reward, policy.player)\n",
        "        elif valid_move and end:\n",
        "          reward = env.reward(player=policy.player)\n",
        "          buffer.push(state, move, None, reward, policy.player)        \n",
        "        elif not valid_move:\n",
        "          end = True\n",
        "          reward = -1\n",
        "          n_unavilable_actions_taken += 1\n",
        "          buffer.push(state, move, None, reward, policy.player)\n",
        "          break\n",
        "        else:\n",
        "          raise ValueError(\"invalid move\")\n",
        "\n",
        "        loss = optimize_model(Q1, Q2, buffer, optimizer)\n",
        "        \n",
        "        if loss is not None:\n",
        "          losses.append(loss)\n",
        "        \n",
        "        state = next_state.copy()\n",
        "\n",
        "    rewards.append(reward)\n",
        "    \n",
        "    if len(rewards) >= LOG_EVERY:\n",
        "        writer.add_scalar(\"Average Reward\", np.mean(rewards), itr)\n",
        "        writer.add_scalar(\"Average Loss\", np.mean(loss), itr)\n",
        "        writer.add_scalar(\"Number of unavilable actions taken\", n_unavilable_actions_taken, itr)\n",
        "        writer.flush()\n",
        "        rewards = []\n",
        "        losses = []\n",
        "        n_unavilable_actions_taken = 0\n",
        "\n",
        "    if (itr) % UPDATE_EVERY == 0:\n",
        "      Q2.load_state_dict(Q1.state_dict())      \n",
        "    num_trained_games += 1\n",
        "    env.reset()\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "EeJYe4Wrt20U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "O7wnSLP5Pz9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 12\n",
        "\n",
        "Repeat the training but without the replay buffer and with a batch size of 1 : At every step, update the network by using only the latest transition. What do you observe?"
      ],
      "metadata": {
        "id": "tMrwuQ5z2BuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.5\n",
        "epsilon_opt = 0.5\n",
        "\n",
        "env = TictactoeEnv()\n",
        "buffer = ReplayBuffer(1, 1)\n",
        "Q1 = DQN(hidden_size=HIDDEN_SIZE)\n",
        "Q2 = DQN(hidden_size=HIDDEN_SIZE)\n",
        "Q2.load_state_dict(Q1.state_dict())\n",
        "optimizer = optim.Adam(Q1.parameters())\n",
        "policy = DeepEpsilonGreedy(epsilon=epsilon, n_actions=9)\n",
        "date = datetime.now(timezone_cet).strftime(\"%b%d_%H-%M\")\n",
        "writer = SummaryWriter(log_dir=f\"runs/{date} Epsilon: {epsilon}, Epsilon opt: {epsilon_opt}, Batch Size: {1}, Buffer Size: {1}\")"
      ],
      "metadata": {
        "id": "7XgUm4Br2M__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_trained_games = 0\n",
        "av_rewards = []\n",
        "av_loss = []"
      ],
      "metadata": {
        "id": "yUQA9XGA2NAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_games = 20000\n",
        "opponent_player = OptimalPlayer(epsilon_opt, player='O')\n",
        "learning_player = 'X'\n",
        "rewards = []\n",
        "losses = []\n",
        "n_unavilable_actions_taken = 0\n",
        "\n",
        "for itr in range(num_games):\n",
        "    env.reset()\n",
        "    state, end, _ = env.observe()\n",
        "\n",
        "    opponent_player.player, learning_player = learning_player, opponent_player.player\n",
        "\n",
        "    if opponent_player.player == 'X':\n",
        "        opponent_move = opponent_player.act(state)\n",
        "        state, end, _ = env.step(opponent_move)\n",
        "\n",
        "    while not end:\n",
        "        move = policy.act(state)\n",
        "        valid_move = env.check_valid(move)\n",
        "        \n",
        "        if valid_move:\n",
        "          next_state, end, _ = env.step(move)\n",
        "\n",
        "        if valid_move and (not end) and (env.current_player == opponent_player.player):\n",
        "          opponent_move = opponent_player.act(next_state)\n",
        "          next_state, end, _ = env.step(opponent_move)\n",
        "          reward = env.reward(player=learning_player)\n",
        "          buffer.push(state, move, next_state, reward)\n",
        "        elif valid_move and end:\n",
        "          reward = env.reward(player=learning_player)\n",
        "          buffer.push(state, move, None, reward)        \n",
        "        elif not valid_move:\n",
        "          end = True\n",
        "          reward = -1\n",
        "          n_unavilable_actions_taken += 1\n",
        "          buffer.push(state, move, None, reward)\n",
        "          break\n",
        "        else:\n",
        "          raise ValueError(\"invalid move\")\n",
        "\n",
        "        loss = optimize_model(Q1, Q2, buffer, optimizer)\n",
        "        \n",
        "        if loss is not None:\n",
        "          losses.append(loss)\n",
        "        \n",
        "        state = next_state.copy()\n",
        "\n",
        "    rewards.append(reward)\n",
        "    \n",
        "    if len(rewards) >= LOG_EVERY:\n",
        "        writer.add_scalar(\"Average Reward\", np.mean(rewards), itr)\n",
        "        writer.add_scalar(\"Average Loss\", np.mean(loss), itr)\n",
        "        writer.add_scalar(\"Number of unavilable actions taken\", n_unavilable_actions_taken, itr)\n",
        "        writer.flush()\n",
        "        rewards = []\n",
        "        losses = []\n",
        "        n_unavilable_actions_taken = 0\n",
        "\n",
        "    if (itr) % UPDATE_EVERY == 0:\n",
        "      Q2.load_state_dict(Q1.state_dict())      \n",
        "    num_trained_games += 1\n",
        "    env.reset()\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "GilGyaiz2NAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "DjqXMsUYdlTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug Code:\n"
      ],
      "metadata": {
        "id": "t-84e7gxQ18e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = DQN()\n",
        "model2 = DQN()"
      ],
      "metadata": {
        "id": "Z2dOVwNr9B1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state1 = np.array([0, 0, 1, 0, -1, -1, 0, 0, 0]).reshape((3, 3))\n",
        "state2 = np.array([0, 1, 1, 0, 1, -1, -1, 0, -1]).reshape((3, 3))\n",
        "batch_size = 7\n",
        "buffer = ReplayBuffer(1000, batch_size)\n",
        "state1 = state_to_tensor(state1)\n",
        "state2 = state_to_tensor(state2)\n",
        "action1 = model1(state1).max(1).indices\n",
        "action2 = model1(state2).max(1).indices\n",
        "for i in range(100):\n",
        "  if i % 3 == 0:\n",
        "    buffer.push(state, action1, state2, torch.tensor([1], device=device))\n",
        "  elif i % 3 == 1:\n",
        "    buffer.push(state2, action2, state1, torch.tensor([1], device=device))\n",
        "  elif i % 3 == 2:\n",
        "    buffer.push(state2, action2, None, torch.tensor([1], device=device))"
      ],
      "metadata": {
        "id": "9i_iGA0EH1EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transitions = buffer.get_batch()\n",
        "batch = Transition(*zip(*transitions))"
      ],
      "metadata": {
        "id": "w9PxRoy4dmsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1(state).max(1).indices"
      ],
      "metadata": {
        "id": "aS3qlc82SsVw",
        "outputId": "1bfed816-3d2b-4a90-f767-e97432ce24d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transitions = buffer.get_batch()\n",
        "batch = Transition(*zip(*transitions))\n",
        "state_batch = torch.stack(batch.state)\n",
        "action_batch = torch.cat(batch.action)\n",
        "reward_batch = torch.cat(batch.reward)\n",
        "non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                      batch.next_state)), device=device, dtype=torch.bool)\n",
        "non_final_next_states = torch.stack([s for s in batch.next_state\n",
        "                                            if s is not None])\n",
        "state_action_values = model1(state_batch).take(action_batch)\n",
        "next_state_values = torch.zeros(action_batch.size()[0], device=device)\n",
        "next_state_values[non_final_mask] = Q2(non_final_next_states).max(1).values.detach()"
      ],
      "metadata": {
        "id": "pOBIGQhCUPhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_final_next_states = state_batch[non_final_mask]\n",
        "non_final_next_states.size()"
      ],
      "metadata": {
        "id": "N4333LtBFRRM",
        "outputId": "394b493c-9515-427e-9bc1-820dbf4adb4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_batch.size(), \\\n",
        "action_batch.size(), \\\n",
        "reward_batch.size(), \\\n",
        "model1(state_batch).max(1).indices.size(), \\\n",
        "action_batch.size(), \\\n",
        "non_final_mask.size(), \\\n",
        "non_final_next_states.size(), \\\n",
        "state_action_values.size(), \\\n",
        "next_state_values.size(),"
      ],
      "metadata": {
        "id": "8z2R_Uylf6-0",
        "outputId": "28a634e7-0f24-463a-9215-c752bc274f7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([7, 3, 3, 2]),\n",
              " torch.Size([7]),\n",
              " torch.Size([7]),\n",
              " torch.Size([7]),\n",
              " torch.Size([7]),\n",
              " torch.Size([7]),\n",
              " torch.Size([5, 3, 3, 2]),\n",
              " torch.Size([7]),\n",
              " torch.Size([7]))"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  model1(state_batch), action_batch, state_action_values"
      ],
      "metadata": {
        "id": "n8J-TPnLtA_n",
        "outputId": "8a7ad670-64ff-4ad6-ed68-77866eacf3a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0286,  0.0695,  0.1031, -0.1010,  0.0176, -0.1728,  0.1158, -0.1029,\n",
              "           0.0879],\n",
              "         [ 0.0200,  0.1495,  0.1172, -0.1278,  0.0041, -0.2172,  0.0348, -0.2061,\n",
              "           0.0445],\n",
              "         [ 0.0200,  0.1495,  0.1172, -0.1278,  0.0041, -0.2172,  0.0348, -0.2061,\n",
              "           0.0445],\n",
              "         [ 0.0286,  0.0695,  0.1031, -0.1010,  0.0176, -0.1728,  0.1158, -0.1029,\n",
              "           0.0879],\n",
              "         [ 0.0286,  0.0695,  0.1031, -0.1010,  0.0176, -0.1728,  0.1158, -0.1029,\n",
              "           0.0879],\n",
              "         [ 0.0200,  0.1495,  0.1172, -0.1278,  0.0041, -0.2172,  0.0348, -0.2061,\n",
              "           0.0445],\n",
              "         [ 0.0286,  0.0695,  0.1031, -0.1010,  0.0176, -0.1728,  0.1158, -0.1029,\n",
              "           0.0879]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([6, 1, 1, 6, 6, 1, 6], device='cuda:0'),\n",
              " tensor([0.1158, 0.0695, 0.0695, 0.1158, 0.1158, 0.0695, 0.1158],\n",
              "        device='cuda:0', grad_fn=<TakeBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "expected_state_action_values"
      ],
      "metadata": {
        "id": "zCjvqyAVAbyJ",
        "outputId": "0d8edc9d-b85b-437d-e142-cb2d4ead6eae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-5.3578,  1.0000, -5.3856, -5.3578, -5.3578,  1.0000, -5.3578],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_action_values"
      ],
      "metadata": {
        "id": "Oa9TDdIMKniH",
        "outputId": "99ca2062-7047-4ae4-f69a-fee41634c5e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1495, 0.0348, 0.0348, 0.0348, 0.1495, 0.1495, 0.1495],\n",
              "       device='cuda:0', grad_fn=<TakeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_action_values.size(), expected_state_action_values.size()"
      ],
      "metadata": {
        "id": "wWzucLB9AwCK",
        "outputId": "a7477834-8eaa-4f3a-ca54-2562291d0632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([7]), torch.Size([7]))"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Huber loss\n",
        "criterion = nn.SmoothL1Loss()\n",
        "loss = criterion(state_action_values, expected_state_action_values)\n",
        "loss"
      ],
      "metadata": {
        "id": "fWP8tyhnAZjg",
        "outputId": "92088210-118f-4c10-8e6d-da5d7d163edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.6736, device='cuda:0', grad_fn=<SmoothL1LossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(loss.item())"
      ],
      "metadata": {
        "id": "aI6EQm4DK4Xa",
        "outputId": "3dfca636-bbde-4833-d600-a71ae0aab81b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "float"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state1 = np.array([1, 1, 1, 1, 1, 1, 0, 0, 0]).reshape((3, 3))\n",
        "Q1(state_to_tensor(state1)), policy.act(state1)"
      ],
      "metadata": {
        "id": "xwCpFBLur2DE",
        "outputId": "4573e60f-4a7c-43e6-ae05-5c2f21173c16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.9851, -0.9883, -1.0014, -1.0223, -0.9901, -1.0177, -0.9982, -1.0083,\n",
              "          -0.9892]], device='cuda:0', grad_fn=<AddmmBackward0>), 0)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Deep_Q-Learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}