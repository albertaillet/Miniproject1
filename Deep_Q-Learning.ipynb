{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertaillet/Miniproject1/blob/master/Deep_Q-Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/albertaillet/Miniproject1/"
      ],
      "metadata": {
        "id": "nwftAcdqziEE",
        "outputId": "8d80f59b-4d8d-4e34-d161-94f0b9cd8bb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Miniproject1' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Miniproject1/"
      ],
      "metadata": {
        "id": "OEMI9QVk1lOd",
        "outputId": "5a17c69f-8323-490d-b87c-7cc4ff5940f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Miniproject1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "SP6liwgV2j1a",
        "outputId": "7f16b400-a060-484e-f93e-f5911f273a37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfZLCGEyzfdm"
      },
      "source": [
        "## 3. Deep Q-Learning\n",
        "\n",
        "As our 2nd algorithm, we use Deep Q-Learning (DQN) combined with $\\epsilon$-greedy policy. You can watch again Part 1 of Deep Reinforcement Learning Lecture 1 for an introduction to DQN and Part 1 of Deep Reinforcement Learning Lecture 2 (in particular slide 8) for more details. The idea in DQN is to approximate $Q$-values by a neural network instead of a look-up table as in Tabular Q-learning. For implementation, you can use ideas from the DQN tutorials of Keras and PyTorch.\n",
        "\n",
        "### 3.1 Implementation details\n",
        "State representation: We represent state $s_{t}$ by a $3 \\times 3 \\times 2$ tensor $x_{-} t$. Each element of $x_{-} t$ takes a value of 0 or 1. The $3 \\times 3$ matrix $x_{t} t[:, z, 0]$ shows positions taken by you, and $x_{-}[:,:, 1]$ shows positions taken by your opponent. If $x_{-} t[i, j, 0]=x_{2} t[i, j, 1]=0$, then position $(i, j)$ is available.\n",
        "Neural network architecture: We use a fully connected network. State $x_{-} t$ is fed to the network at the input layer. We consider 2 hidden layers each with 128 neurons - with ReLu activation functions. The output layer has 9 neurons (for 9 different actions) with linear activation functions. Each neuron at the output layer shows the $Q$-value of the corresponding action at state $x_{-} t$.\n",
        "\n",
        "Unavailable actions: For DQN, we do not constraint actions to only available actions. However, whenever the agent takes an unavailable action, we end the game and give the agent a negative reward of value $r_{\\text {unav }}=-1$.\n",
        "\n",
        "Free parameters: DQN has many hyper parameters. For convenience, we fix the discount factor at $\\gamma=0.99$. We assume a buffer size of $10^{\\prime} 000$ and a batch size of 64 . We update the target network every 500 games. Instead of squared loss, we use the Huber loss (with $\\delta=1$ ) with Adam optimizer (c.f. the DQN tutorials of Keras and PyTorch). You can fine tune the learning rate if needed, but we suggest $5 \\times 10^{-4}$ as a starting point.\n",
        "\n",
        "Other options? There are tens of different ways to make training of deep networks more efficient. Do you feel like trying some and learning more? You are welcome to do so; you just need to explain the main features of your implementation and a brief summary of your reasoning in less than 300 words under the title 'Implementation details' in your report.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        input_size = 9\n",
        "        hidden_size = 128\n",
        "        output_size = 9\n",
        "        \n",
        "        self.input_to_hidden1 = nn.Linear(input_size, hidden_size)\n",
        "        self.hidden1_to_hidden2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.hidden2_to_output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = x.flatten()\n",
        "        x = F.relu(self.input_to_hidden1(x))\n",
        "        x = F.relu(self.hidden1_to_hidden2(x))\n",
        "        x = F.relu(self.hidden2_to_output(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "l8AvlQV12UqC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = DQN()"
      ],
      "metadata": {
        "id": "W-Kg9Q138InN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand((3, 3))\n",
        "dqn(a)"
      ],
      "metadata": {
        "id": "1YVUMrAX8K_A",
        "outputId": "8d1ed0c2-688b-4da6-8701-b76ec331fe97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0529, 0.0000, 0.0000, 0.0000, 0.0030, 0.0442, 0.0000, 0.0056, 0.0000],\n",
              "       grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.05 # learning rate \n",
        "gamma = 0.99 # discount factor\n",
        "buffer_size = 10000\n",
        "batch_size = 64\n",
        "update_every = 500"
      ],
      "metadata": {
        "id": "bfa8g1X89Ia5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Deep_Q-Learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}