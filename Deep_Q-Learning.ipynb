{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertaillet/Miniproject1/blob/master/Deep_Q-Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If using the notebook on [Colab](https://colab.research.google.com/), run this cell to get the correct files downloaded."
      ],
      "metadata": {
        "id": "I54R3y7XD2J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if \"Miniproject1\" not in os.getcwd():\n",
        "  !git clone https://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@github.com/albertaillet/Miniproject1/\n",
        "  %cd Miniproject1/\n",
        "%ls"
      ],
      "metadata": {
        "id": "nwftAcdqziEE",
        "outputId": "d927a50e-bf04-47d5-ca33-ac11cb8aa3ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Miniproject1'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 119 (delta 46), reused 96 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (119/119), 2.84 MiB | 1.12 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "/content/Miniproject1\n",
            "Deep_Q-Learning.ipynb    plot_table.py         tic_env.py\n",
            "\u001b[0m\u001b[01;34mimages\u001b[0m/                  Q-learning_DAM.ipynb  tic_tac_toe.ipynb\n",
            "MP_TicTocToe.pdf         Q-learning.ipynb\n",
            "performance_measures.py  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from collections import namedtuple, deque\n",
        "from tic_env import TictactoeEnv, OptimalPlayer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "SP6liwgV2j1a",
        "outputId": "2d49a727-4039-4f57-957d-394579f3f84c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfZLCGEyzfdm"
      },
      "source": [
        "## 3. Deep Q-Learning\n",
        "\n",
        "As our 2nd algorithm, we use Deep Q-Learning (DQN) combined with $\\epsilon$-greedy policy. You can watch again Part 1 of Deep Reinforcement Learning Lecture 1 for an introduction to DQN and Part 1 of Deep Reinforcement Learning Lecture 2 (in particular slide 8) for more details. The idea in DQN is to approximate $Q$-values by a neural network instead of a look-up table as in Tabular Q-learning. For implementation, you can use ideas from the DQN tutorials of [Keras](https://keras.io/examples/rl/deep_q_network_breakout/) and [PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Implementation details\n",
        "State representation: We represent state $s_{t}$ by a $3 \\times 3 \\times 2$ tensor `x_t`. Each element of `x_t` takes a value of 0 or 1. The $3 \\times 3$ matrix `x_t[:,:,0]` shows positions taken by you, and `x_t[:,:,1]` shows positions taken by your opponent. If `x_t[i, j, 0]=x_t[i, j, 1]=0`, then position $(i, j)$ is available.\n",
        "Neural network architecture: We use a fully connected network. State `x_t` is fed to the network at the input layer. We consider 2 hidden layers each with 128 neurons - with ReLu activation functions. The output layer has 9 neurons (for 9 different actions) with linear activation functions. Each neuron at the output layer shows the $Q$-value of the corresponding action at state `x_t`.\n",
        "\n",
        "Unavailable actions: For DQN, we do not constraint actions to only available actions. However, whenever the agent takes an unavailable action, we end the game and give the agent a negative reward of value $r_{\\text {unav }}=-1$.\n",
        "\n",
        "Free parameters: DQN has many hyper parameters. For convenience, we fix the discount factor at $\\gamma=0.99$. We assume a buffer size of $10^{\\prime} 000$ and a batch size of 64 . We update the target network every 500 games. Instead of squared loss, we use the Huber loss (with $\\delta=1$ ) with Adam optimizer (c.f. the DQN tutorials of Keras and PyTorch). You can fine tune the learning rate if needed, but we suggest $5 \\times 10^{-4}$ as a starting point.\n",
        "\n",
        "Other options? There are tens of different ways to make training of deep networks more efficient. Do you feel like trying some and learning more? You are welcome to do so; you just need to explain the main features of your implementation and a brief summary of your reasoning in less than 300 words under the title 'Implementation details' in your report."
      ],
      "metadata": {
        "id": "VgXbm8jzkJnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parameters\n",
        "\n",
        "HIDDEN_SIZE = 128 #@param {type:\"integer\"}\n",
        "GAMMA = 0.99  #@param {type:\"number\"}\n",
        "ALPHA = 5e-4  #@param {type:\"number\"}\n",
        "BUFFER_SIZE = 10000 #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 64 #@param {type:\"integer\"}\n",
        "UPDATE_EVERY = 500 #@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "wWR32S7jjYqO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size=18, hidden_size=128, output_size=9):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = x.view(-1, 3, 3, 2)\n",
        "        return self.seq(x)"
      ],
      "metadata": {
        "id": "l8AvlQV12UqC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates a class for the replay buffer and uses code from [Replay Memory](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#replay-memory)"
      ],
      "metadata": {
        "id": "QH-hWdA4pVTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.buffer = deque([], maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.buffer.append(Transition(*args))\n",
        "\n",
        "    def get_batch(self, batch_size=None):\n",
        "        if batch_size is None:\n",
        "          batch_size = self.batch_size\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def has_one_batch(self, batch_size=None):\n",
        "        if batch_size is None:\n",
        "          batch_size = self.batch_size\n",
        "        return len(self) >= batch_size"
      ],
      "metadata": {
        "id": "NkZKkba1kg95"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates a function to call to optimize the model and uses code from \n",
        "[Training loop](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training-loop)"
      ],
      "metadata": {
        "id": "3xpSUJg4vJk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model(model: nn.Module, buffer: ReplayBuffer, optimizer: optim.Optimizer):\n",
        "    if not buffer.has_one_batch():\n",
        "        return\n",
        "    transitions = buffer.get_batch()\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.stack(batch.state)\n",
        "    action_batch = torch.stack(batch.action)\n",
        "    reward_batch = torch.stack(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = model(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0].detach()\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in model.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "mrhPgJm5uSBk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepEpsilonGreedy:\n",
        "    def __init__(self, epsilon=0, n_actions=9):\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "    \n",
        "    def set_epsilon(self, epsilon):\n",
        "        self.epsilon = epsilon\n",
        "    \n",
        "    def act(self, state):\n",
        "      if np.random.random() > self.epsilon:\n",
        "          with torch.no_grad():\n",
        "              return model(state).max(1).indices\n",
        "      else:\n",
        "          return torch.tensor([random.randrange(self.n_actions)], device=device, dtype=torch.long)"
      ],
      "metadata": {
        "id": "v3Mrwx7_FDkV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def state_to_tensor(state):\n",
        "  t = np.zeros((3, 3, 2), dtype=np.int8)\n",
        "  t[:, :, 0] = (state == 1)\n",
        "  t[:, :, 1] = (state ==-1)\n",
        "  return torch.tensor(t, dtype=torch.float32, device=device)"
      ],
      "metadata": {
        "id": "n5PtNeGQKG7g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TictactoeEnv()\n",
        "buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "model = DQN(hidden_size=HIDDEN_SIZE)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "epsilon_greedy = DeepEpsilonGreedy(epsilon=0.1, n_actions=9)"
      ],
      "metadata": {
        "id": "W-Kg9Q138InN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = []\n",
        "num_trained_games = 0\n",
        "av_rewards = []"
      ],
      "metadata": {
        "id": "KaSLPKwbt8Ep"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_games = 20000\n",
        "opponent_player = OptimalPlayer(0.5, player='O')\n",
        "learning_player = 'X'\n",
        "rewards = []\n",
        "\n",
        "for itr in range(num_games):\n",
        "    env.reset()\n",
        "    state, end, _ = env.observe()\n",
        "\n",
        "    opponent_player.player, learning_player = learning_player, opponent_player.player\n",
        "\n",
        "    if opponent_player.player == 'X':\n",
        "        opponent_move = opponent_player.act(state)\n",
        "        state, end, _ = env.step(opponent_move)\n",
        "\n",
        "    while not end:\n",
        "        move = epsilon_greedy.act(state_to_tensor(state))\n",
        "        valid_move = env.check_valid(move.item())\n",
        "        if valid_move:\n",
        "          next_state, end, _ = env.step(move.item())\n",
        "\n",
        "        if valid_move and (not end) and (env.current_player == opponent_player.player):\n",
        "            opponent_move = opponent_player.act(next_state)\n",
        "            next_state, end, _ = env.step(opponent_move)\n",
        "        \n",
        "        reward = env.reward(player=learning_player)\n",
        "        if not valid_move:\n",
        "          reward = -1\n",
        "\n",
        "        buffer.push(state_to_tensor(state), \n",
        "                    move, \n",
        "                    state_to_tensor(next_state), \n",
        "                    torch.tensor([reward], device=device))\n",
        "\n",
        "        if not valid_move:\n",
        "          break\n",
        "        \n",
        "        state = next_state  \n",
        "\n",
        "    rewards.append(reward)\n",
        "    if len(rewards) >= 250:\n",
        "        av_rewards.append(np.mean(rewards))\n",
        "        rewards = []\n",
        "        clear_output(wait=True)\n",
        "        plt.plot(av_rewards)\n",
        "        plt.show()\n",
        "\n",
        "    if (itr + 1) % UPDATE_EVERY == 0:\n",
        "      optimize_model(model, buffer, optimizer)\n",
        "      print(f\"optimization done, buffer: {len(buffer)}\")\n",
        "    num_trained_games += 1\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "EeJYe4Wrt20U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug Code:\n"
      ],
      "metadata": {
        "id": "t-84e7gxQ18e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = np.array([0, 0, 1, 0, -1, -1, 0, 0, 0]).reshape((3, 3))\n",
        "buffer = ReplayBuffer(1000, 3)\n",
        "state = state_to_tensor(state)\n",
        "for i in range(10):\n",
        "  buffer.push(state, torch.tensor([1], device=device), state, torch.tensor([1], device=device))"
      ],
      "metadata": {
        "id": "9i_iGA0EH1EN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transitions = buffer.get_batch()\n",
        "batch = Transition(*zip(*transitions))"
      ],
      "metadata": {
        "id": "w9PxRoy4dmsy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_batch = torch.stack(batch.state)\n",
        "f = nn.Flatten()\n",
        "state_batch.shape, state_batch.flatten(start_dim=1).shape, f(state_batch).shape"
      ],
      "metadata": {
        "id": "8gxw_GJLdsyS",
        "outputId": "adb4cf9c-9649-4753-ff10-362b6bdc40da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 3, 3, 2]), torch.Size([3, 18]), torch.Size([3, 18]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DQN()"
      ],
      "metadata": {
        "id": "SHnIJwWBSouv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(state).max(1).indices"
      ],
      "metadata": {
        "id": "aS3qlc82SsVw",
        "outputId": "a1ef79ee-3fb8-4478-d3bf-d031114e92b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(state_batch).max(1).indices"
      ],
      "metadata": {
        "id": "D3cfW2GDK-jZ",
        "outputId": "4c5d6259-db23-4d2e-f3f5-6cbd47f93753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6, 6, 6], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pOBIGQhCUPhl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Deep_Q-Learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}