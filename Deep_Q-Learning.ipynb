{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertaillet/Miniproject1/blob/master/Deep_Q-Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if \"Miniproject1\" not in os.getcwd():\n",
        "  !git clone https://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@github.com/albertaillet/Miniproject1/\n",
        "  %cd Miniproject1/\n",
        "%ls"
      ],
      "metadata": {
        "id": "nwftAcdqziEE",
        "outputId": "4aa2fb16-294f-449f-8a41-1f26ab90cfe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep_Q-Learning.ipynb    plot_table.py         README.md\n",
            "\u001b[0m\u001b[01;34mimages\u001b[0m/                  \u001b[01;34m__pycache__\u001b[0m/          tic_env.py\n",
            "MP_TicTocToe.pdf         Q-learning_DAM.ipynb  tic_tac_toe.ipynb\n",
            "performance_measures.py  Q-learning.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from tic_env import TictactoeEnv, OptimalPlayer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "SP6liwgV2j1a",
        "outputId": "28d4cc01-a431-410e-92b9-9e8ddd2fb374",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfZLCGEyzfdm"
      },
      "source": [
        "## 3. Deep Q-Learning\n",
        "\n",
        "As our 2nd algorithm, we use Deep Q-Learning (DQN) combined with $\\epsilon$-greedy policy. You can watch again Part 1 of Deep Reinforcement Learning Lecture 1 for an introduction to DQN and Part 1 of Deep Reinforcement Learning Lecture 2 (in particular slide 8) for more details. The idea in DQN is to approximate $Q$-values by a neural network instead of a look-up table as in Tabular Q-learning. For implementation, you can use ideas from the DQN tutorials of [Keras](https://keras.io/examples/rl/deep_q_network_breakout/) and [PyTorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Implementation details\n",
        "State representation: We represent state $s_{t}$ by a $3 \\times 3 \\times 2$ tensor `x_t`. Each element of `x_t` takes a value of 0 or 1. The $3 \\times 3$ matrix `x_t[:,:,0]` shows positions taken by you, and `x_t[:,:,1]` shows positions taken by your opponent. If `x_t[i, j, 0]=x_t[i, j, 1]=0`, then position $(i, j)$ is available.\n",
        "Neural network architecture: We use a fully connected network. State `x_t` is fed to the network at the input layer. We consider 2 hidden layers each with 128 neurons - with ReLu activation functions. The output layer has 9 neurons (for 9 different actions) with linear activation functions. Each neuron at the output layer shows the $Q$-value of the corresponding action at state `x_t`.\n",
        "\n",
        "Unavailable actions: For DQN, we do not constraint actions to only available actions. However, whenever the agent takes an unavailable action, we end the game and give the agent a negative reward of value $r_{\\text {unav }}=-1$.\n",
        "\n",
        "Free parameters: DQN has many hyper parameters. For convenience, we fix the discount factor at $\\gamma=0.99$. We assume a buffer size of $10^{\\prime} 000$ and a batch size of 64 . We update the target network every 500 games. Instead of squared loss, we use the Huber loss (with $\\delta=1$ ) with Adam optimizer (c.f. the DQN tutorials of Keras and PyTorch). You can fine tune the learning rate if needed, but we suggest $5 \\times 10^{-4}$ as a starting point.\n",
        "\n",
        "Other options? There are tens of different ways to make training of deep networks more efficient. Do you feel like trying some and learning more? You are welcome to do so; you just need to explain the main features of your implementation and a brief summary of your reasoning in less than 300 words under the title 'Implementation details' in your report."
      ],
      "metadata": {
        "id": "VgXbm8jzkJnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parameters\n",
        "\n",
        "INPUT_SIZE = 18 #@param {type:\"integer\"}\n",
        "HIDDEN_SIZE = 128 #@param {type:\"integer\"}\n",
        "OUTPUT_SIZE = 9 #@param {type:\"integer\"}\n",
        "GAMMA = 0.99  #@param {type:\"number\"}\n",
        "ALPHA = 5e-4  #@param {type:\"number\"}\n",
        "BUFFER_SIZE = 10000 #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 64 #@param {type:\"integer\"}\n",
        "UPDATE_EVERY = 500 #@param {type:\"integer\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wWR32S7jjYqO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()        \n",
        "        self.input_to_hidden1 = nn.Linear(input_size, hidden_size)\n",
        "        self.hidden1_to_hidden2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.hidden2_to_output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = x.flatten()\n",
        "        x = self.input_to_hidden1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.hidden1_to_hidden2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.hidden2_to_output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "l8AvlQV12UqC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates a class for the replay buffer and uses code from [Replay Memory](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#replay-memory)"
      ],
      "metadata": {
        "id": "QH-hWdA4pVTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.buffer = deque([],maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.buffer.append(Transition(*args))\n",
        "\n",
        "    def get_batch(self, batch_size=None):\n",
        "        if batch_size is None:\n",
        "          batch_size = self.batch_size\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def has_one_batch(self, batch_size=None):\n",
        "        if batch_size is None:\n",
        "          batch_size = self.batch_size\n",
        "        return len(self) >= batch_size"
      ],
      "metadata": {
        "id": "NkZKkba1kg95"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates a function to call to optimize the model and uses code from \n",
        "[Training loop](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training-loop)"
      ],
      "metadata": {
        "id": "3xpSUJg4vJk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model(model: nn.Module, buffer: ReplayBuffer, optimizer: optim.Optimizer):\n",
        "    if not buffer.has_one_batch():\n",
        "        return\n",
        "    transitions = buffer.get_batch()\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = model(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0].detach()\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in model.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "mrhPgJm5uSBk"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepEpsilonGreedy:\n",
        "    def __init__(self, epsilon=0, n_actions=9):\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "    \n",
        "    def set_epsilon(self, epsilon):\n",
        "        self.epsilon = epsilon\n",
        "    \n",
        "    def act(self, state):\n",
        "      if np.random.random() > self.epsilon:\n",
        "          with torch.no_grad():\n",
        "              # t.max(1) will return largest column value of each row.\n",
        "              # second column on max result is index of where max element was\n",
        "              # found, so we pick action with the larger expected reward.\n",
        "              return model(state).max(0).indices.view(1)\n",
        "      else:\n",
        "          return torch.tensor([random.randrange(self.n_actions)], device=device, dtype=torch.long)"
      ],
      "metadata": {
        "id": "v3Mrwx7_FDkV"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def state_to_tensor(state):\n",
        "  t = np.zeros((3, 3, 2), dtype=np.int8)\n",
        "  t[:, :, 0] = (state == 1)\n",
        "  t[:, :, 1] = (state ==-1)\n",
        "  return torch.tensor(t, dtype=torch.float32, device=device)"
      ],
      "metadata": {
        "id": "n5PtNeGQKG7g"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TictactoeEnv()\n",
        "buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "model = DQN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "epsilon_greedy = DeepEpsilonGreedy(epsilon=0.1, n_actions=OUTPUT_SIZE)"
      ],
      "metadata": {
        "id": "W-Kg9Q138InN"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = []\n",
        "num_trained_games = 0\n",
        "av_rewards = []"
      ],
      "metadata": {
        "id": "KaSLPKwbt8Ep"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_games = 20000\n",
        "opponent_player = OptimalPlayer(0.5, player='O')\n",
        "learning_player = 'X'\n",
        "rewards = []\n",
        "\n",
        "for itr in range(num_games):\n",
        "    env.reset()\n",
        "    state, end, _ = env.observe()\n",
        "\n",
        "    opponent_player.player, learning_player = learning_player, opponent_player.player\n",
        "\n",
        "    if opponent_player.player == 'X':\n",
        "        opponent_move = opponent_player.act(state)\n",
        "        state, end, _ = env.step(opponent_move)\n",
        "\n",
        "    while not end:\n",
        "        move = epsilon_greedy.act(state_to_tensor(state))\n",
        "        valid_move = env.check_valid(move.item())\n",
        "        if valid_move:\n",
        "          next_state, end, _ = env.step(move.item())\n",
        "\n",
        "        if valid_move and (not end) and (env.current_player == opponent_player.player):\n",
        "            opponent_move = opponent_player.act(next_state)\n",
        "            next_state, end, _ = env.step(opponent_move)\n",
        "        \n",
        "        reward = env.reward(player=learning_player)\n",
        "        if not valid_move:\n",
        "          reward = -1\n",
        "\n",
        "        buffer.push(state_to_tensor(state), \n",
        "                    move, \n",
        "                    state_to_tensor(next_state), \n",
        "                    torch.tensor([reward], device=device))\n",
        "            \n",
        "        optimize_model(model, buffer, optimizer)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    rewards.append(reward)\n",
        "    if len(rewards) >= 50:\n",
        "        av_rewards.append(np.mean(rewards))\n",
        "        rewards = []\n",
        "        plt.plot(av_rewards)\n",
        "        plt.show()\n",
        "    num_trained_games += 1\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "EeJYe4Wrt20U",
        "outputId": "12e434a9-66e4-43d2-8f7d-cb1992f53bc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-228-50eae4c47413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                     torch.tensor([reward], device=device))\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-163-9756e029ff1d>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(model, buffer, optimizer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# columns of actions taken. These are the actions which would've been taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# for each batch state according to policy_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Compute V(s_{t+1}) for all next states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-cd40afa75bdf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_to_hidden1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden1_to_hidden2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1152 and 18x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = np.array([0, 0, 1, 0, -1, -1, 0, 0, 0]).reshape((3, 3))\n",
        "state_to_tensor(state).dtype\n",
        "a = model(state_to_tensor(state)).max(0).indices.view(1).item()\n",
        "a"
      ],
      "metadata": {
        "id": "9i_iGA0EH1EN",
        "outputId": "b60373c5-54c0-48b6-8b0c-668fa0d2233c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "64*18"
      ],
      "metadata": {
        "id": "D3cfW2GDK-jZ",
        "outputId": "0241723f-1265-4e1d-e155-c5dfe091e6fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1152"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Deep_Q-Learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}