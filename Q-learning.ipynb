{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tic_plot import plot_grid\n",
    "from IPython.display import clear_output\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from performance_measures import M_opt, M_rand\n",
    "from q_table import QTable\n",
    "from policies import EpsilonGreedy, EpsilonGreedyDecreasingExploration\n",
    "\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M_{\\mathrm{opt}}$ measures the performance of $\\pi$ against the optimal policy. <br>\n",
    "To compute $M_{\\mathrm{opt}}$, we run $\\pi$ against `Opt(0)` for `N = 500` games for different random seeds. <br>\n",
    "$\\pi$ makes the 1st move in 250 games, and `Opt(0)` makes the 1st move in the rest. <br>\n",
    "We count how many games $\\pi$ wins ($N_{\\mathrm{win}}$) and loses ($N_{\\mathrm{loss}}$) and define $$M_{\\mathrm{opt}} = \\frac{N_{\\mathrm{win}} - N_{\\mathrm{loss}}}{N}$$.\n",
    "\n",
    "$M_{\\mathrm{rand}}$ measures the performance of against the random policy. <br>\n",
    "To compute $M_{\\mathrm{rand}}$, we repeat what we did for computing $M_{\\mathrm{opt}}$ but by using `Opt(1)` instead of `Opt(0)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning combined with epsilon-greedy policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time $t$, state $s_t$ is the board position, action $a_t$ is one of the available positions on the board and reward $r_t$. <br>\n",
    "The reward only non-zero when the game ends where you get $r_t = 1$ if you win the game, $r_t = 1$ if you lose, and $r_t=0$ if it is a draw.\n",
    "\n",
    "Q-Learning has 3 hyper-parameters: \n",
    "- learning rate $\\alpha$\n",
    "- discount factor $\\gamma$\n",
    "- and exploration level $\\epsilon$ \n",
    "\n",
    "For convenience, we fix the learning rate at alpha = 0.05 and the discount factor at $\\gamma$ = 0.99. <br>\n",
    "We initialize all the Q-values at 0. <br>\n",
    "If you are curious, you can explore the effect of $\\alpha$, $\\gamma$ and initial Q-values for yourself. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 # learning rate \n",
    "gamma = 0.99 # discount factor\n",
    "epsilon = 0.1 # exploration rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Learning from experts\n",
    "\n",
    "In this section, you will study whether Q-learning can learn to play Tic Tac Toe by playing against `Opt(`$\\epsilon_{opt}$`)` for some $\\epsilon_{opt} \\in [0; 1]$. <br> \n",
    "To do so, implement the Q-learning algorithm. To check the algorithm, run a Q-learning agent, with a fixed and arbitrary $\\epsilon_{opt} \\in [0; 1)$ against `Opt(0.5)` for 20'000 games. <br> \n",
    "Switch the 1st player after every game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()\n",
    "Q_table = QTable()\n",
    "rewards = []\n",
    "num_trained_games = 0\n",
    "av_rewards = []\n",
    "\n",
    "policy = EpsilonGreedy(Q_table, epsilon=epsilon)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 20000\n",
    "opponent_player = OptimalPlayer(0.5, player='O')\n",
    "learning_player = 'X'\n",
    "rewards = []\n",
    "\n",
    "for itr in range(num_games):\n",
    "    env.reset()\n",
    "    state, end, _ = env.observe()\n",
    "\n",
    "    opponent_player.player, learning_player = learning_player, opponent_player.player\n",
    "\n",
    "    if opponent_player.player == 'X':\n",
    "        opponent_move = opponent_player.act(state)\n",
    "        state, end, _ = env.step(opponent_move)\n",
    "\n",
    "    while not end:\n",
    "        move = policy.act(state)\n",
    "        next_state, end, _ = env.step(move)\n",
    "\n",
    "        if (not end) and (env.current_player == opponent_player.player):\n",
    "            opponent_move = opponent_player.act(next_state)\n",
    "            next_state, end, _ = env.step(opponent_move)\n",
    "        \n",
    "        reward = env.reward(player=learning_player)\n",
    "        \n",
    "        Q_table[state][move] += alpha * (reward + gamma * max(Q_table[next_state].values()) - Q_table[state][move])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    rewards.append(reward)\n",
    "    if len(rewards) >= 250:\n",
    "        av_rewards.append(np.mean(rewards))\n",
    "        rewards = []\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(av_rewards)\n",
    "        plt.show()\n",
    "    num_trained_games += 1\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './runs/Q1/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "with open(os.path.join(log_dir, 'av_rewards.pkl'), 'wb+') as f:\n",
    "    pickle.dump(av_rewards, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-paper')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(0, num_trained_games, 250), av_rewards, \"-\")\n",
    "plt.title(f\"Average reward for every 250 games during training\\nLearning rate {alpha:.2f}, Discount factor: {gamma:.2f}, Exploration rate: {epsilon:.2f}\")\n",
    "plt.ylabel(\"Avergage reward per 250 games\")\n",
    "plt.xlabel(\"Number of training games\")\n",
    "plt.grid(color=\"lightgrey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Decreasing exploration\n",
    "\n",
    "One way to make training more efficient is to decrease the exploration level $\\epsilon$ over time. If we define $\\epsilon(n)$ to be $\\epsilon$ for game number $n$, then one feasible way to decrease exploration during training is to use\n",
    "$$\n",
    "\\epsilon(n)=\\max \\left\\{\\epsilon_{\\min }, \\epsilon_{\\max }\\left(1-\\frac{n}{n^{*}}\\right)\\right\\}\n",
    "$$\n",
    "where $\\epsilon_{\\min }$ and $\\epsilon_{\\max }$ are the minimum and maximum values for $\\epsilon$, respectively, and $n^{*}$ is the number of exploratory games and shows how fast $\\epsilon$ decreases. For convenience, we assume $\\epsilon_{\\min }=0.1$ and $\\epsilon_{\\max }=0.8$; if you are curious, you can explore their effect on performance for yourself. Use $\\epsilon(n)$ as define above and run different $Q$-learning agents with different values of $n^{*}$ against `Opt(0.5)` for 20'000 games - switch the 1st player after every game. Choose several values of $n^{*}$ from a reasonably wide interval between 1 to $40^{\\prime} 000$ - particularly, include $n^{*}=1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 # learning rate \n",
    "gamma = 0.99 # discount factor\n",
    "epsilon_min, epsilon_max = 0.1, 0.8 # exploration rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_rewards_dict = {}\n",
    "m_opt_dict = {}\n",
    "m_rand_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 20000\n",
    "\n",
    "for n_star in (1, 5_000, 10_000, 20_000, 40_000):\n",
    "    rewards = []\n",
    "    opponent_player = OptimalPlayer(0.5, player='O')\n",
    "    learning_player = 'X'\n",
    "    av_rewards = []\n",
    "    m_opt_list = []\n",
    "    m_rand_list = []\n",
    "    Q_table = QTable()\n",
    "    policy = EpsilonGreedyDecreasingExploration(Q_table,\n",
    "                                                epsilon_min=epsilon_min, \n",
    "                                                epsilon_max=epsilon_max, \n",
    "                                                n_star=n_star)\n",
    "    env = TictactoeEnv()\n",
    "    \n",
    "    rewards = []\n",
    "    num_trained_games = 0\n",
    "    av_rewards = []\n",
    "\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for itr in range(num_games):\n",
    "        env.reset()\n",
    "        state, end, _ = env.observe()\n",
    "\n",
    "        opponent_player.player, learning_player = learning_player, opponent_player.player\n",
    "\n",
    "        if opponent_player.player == 'X':\n",
    "            opponent_move = opponent_player.act(state)\n",
    "            state, end, _ = env.step(opponent_move)\n",
    "\n",
    "        while not end:\n",
    "            policy.update_epsilon(itr)\n",
    "            move = policy.act(state)\n",
    "            next_state, end, _ = env.step(move)\n",
    "\n",
    "            if (not end) and (env.current_player == opponent_player.player):\n",
    "                opponent_move = opponent_player.act(next_state)\n",
    "                next_state, end, _ = env.step(opponent_move)\n",
    "            \n",
    "            reward = env.reward(player=learning_player)\n",
    "            \n",
    "            Q_table[state][move] += alpha * (reward + gamma * max(Q_table[next_state].values()) - Q_table[state][move])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(reward)\n",
    "        if len(rewards) >= 250:\n",
    "            av_rewards.append(np.mean(rewards))\n",
    "            rewards = []\n",
    "            clear_output(wait=True)\n",
    "            plt.plot(av_rewards)\n",
    "            plt.show()\n",
    "            policy.set_epsilon(0)\n",
    "            m_opt_list.append(M_opt(policy))\n",
    "            m_rand_list.append(M_rand(policy))\n",
    "        num_trained_games += 1\n",
    "        env.reset()\n",
    "    av_rewards_dict[n_star] = av_rewards\n",
    "    m_opt_dict[n_star] = m_opt_list\n",
    "    m_rand_dict[n_star] = m_rand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './runs/Q2/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "with open(os.path.join(log_dir, 'av_rewards_dict.pkl'), 'wb+') as f:\n",
    "    pickle.dump(av_rewards_dict, f)\n",
    "\n",
    "with open(os.path.join(log_dir, 'm_opt_dict.pkl'), 'wb+') as f:\n",
    "    pickle.dump(m_opt_dict, f)\n",
    "\n",
    "with open(os.path.join(log_dir, 'm_rand_dict.pkl'), 'wb+') as f:\n",
    "    pickle.dump(m_rand_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-paper')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for n_star, av_rewards in sorted(av_rewards_dict.items()):\n",
    "    plt.plot(np.arange(0, num_trained_games, 250), av_rewards, \"-\", label=f\"$n*={n_star}$\")\n",
    "plt.title(f\"Average reward for every 250 games during training\\nLearning rate {alpha:.2f}, Discount factor: {gamma:.2f},\"+\" $\\epsilon_{min}$: \"+str(epsilon_min)+\" $\\epsilon_{max}$: \"+str(epsilon_max))\n",
    "plt.ylabel(\"Avergage reward per 250 games\")\n",
    "plt.xlabel(\"Number of training games\")\n",
    "plt.legend()\n",
    "plt.grid(color=\"lightgrey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-paper')\n",
    "plt.figure(figsize=(10, 5))\n",
    "for n_star, m_opt_list in sorted(m_opt_dict.items()):\n",
    "    plt.plot(np.arange(0, num_trained_games, 250), m_opt_list, \"-\", label=f\"$n*={n_star}$\")\n",
    "plt.title(\"$M_{opt}$\" + f\" for every 250 games during training\\nLearning rate {alpha:.2f}, Discount factor: {gamma:.2f},\"+\" $\\epsilon_{min}$: \"+str(epsilon_min)+\" $\\epsilon_{max}$: \"+str(epsilon_max))\n",
    "plt.ylabel(\"$M_{opt}$ for every 250 games\")\n",
    "plt.xlabel(\"Number of training games\")\n",
    "plt.legend()\n",
    "plt.grid(color=\"lightgrey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-paper')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for n_star, m_rand_list in sorted(m_rand_dict.items()):\n",
    "    plt.plot(np.arange(0, num_trained_games, 250), m_rand_list, \"-\", label=f\"$n*={n_star}$\")\n",
    "plt.title(\"$M_{rand}$\" + f\" for every 250 games during training\\nLearning rate {alpha:.2f}, Discount factor: {gamma:.2f},\"+\" $\\epsilon_{min}$: \"+str(epsilon_min)+\" $\\epsilon_{max}$: \"+str(epsilon_max))\n",
    "plt.ylabel(\"$M_{rand}$ for every 250 games\")\n",
    "plt.xlabel(\"Number of training games\")\n",
    "plt.legend()\n",
    "plt.grid(color=\"lightgrey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Good experts and bad experts\n",
    "Choose the best value of $n^{*}$ that you found in the previous section. Run $Q$-learning against Opt $\\left(\\epsilon_{\\mathrm{opt}}\\right)$ for different values of $\\epsilon_{\\mathrm{opt}}$ for 20 '000 games $-$ switch the 1st player after every game. Choose several values of $\\epsilon_{\\mathrm{opt}}$ from a reasonably wide interval between 0 to $1-$ particularly, include $\\epsilon_{\\mathrm{opt}}=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 # learning rate \n",
    "gamma = 0.99 # discount factor\n",
    "epsilon_min, epsilon_max = 0.1, 0.8 # exploration rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_opt_per_epsilon = {}\n",
    "m_rand_per_epsilon = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_star = 10000\n",
    "num_games = 20000\n",
    "\n",
    "for epsilon_opt in (0, 0.25, 0.50, 0.75, 1.0):\n",
    "    rewards = []\n",
    "    opponent_player = OptimalPlayer(epsilon_opt, player='O')\n",
    "    learning_player = 'X'\n",
    "    av_rewards = []\n",
    "    m_opt_list = []\n",
    "    m_rand_list = []\n",
    "    Q_table = QTable()\n",
    "    policy = EpsilonGreedyDecreasingExploration(Q_table,\n",
    "                                                epsilon_min=epsilon_min, \n",
    "                                                epsilon_max=epsilon_max, \n",
    "                                                n_star=n_star)\n",
    "    env = TictactoeEnv()\n",
    "    \n",
    "    num_trained_games = 0\n",
    "\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for itr in range(num_games):\n",
    "        env.reset()\n",
    "        state, end, _ = env.observe()\n",
    "\n",
    "        opponent_player.player, learning_player = learning_player, opponent_player.player\n",
    "\n",
    "        if opponent_player.player == 'X':\n",
    "            opponent_move = opponent_player.act(state)\n",
    "            state, end, _ = env.step(opponent_move)\n",
    "\n",
    "        while not end:\n",
    "            policy.update_epsilon(itr)\n",
    "            move = policy.act(state)\n",
    "            next_state, end, _ = env.step(move)\n",
    "\n",
    "            if (not end) and (env.current_player == opponent_player.player):\n",
    "                opponent_move = opponent_player.act(next_state)\n",
    "                next_state, end, _ = env.step(opponent_move)\n",
    "            \n",
    "            reward = env.reward(player=learning_player)\n",
    "            \n",
    "            Q_table[state][move] += alpha * (reward + gamma * max(Q_table[next_state].values()) - Q_table[state][move])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(reward)\n",
    "        if len(rewards) >= 250:\n",
    "            av_rewards.append(np.mean(rewards))\n",
    "            rewards = []\n",
    "            policy.set_epsilon(0)\n",
    "            m_opt_list.append(M_opt(policy))\n",
    "            m_rand_list.append(M_rand(policy))\n",
    "            clear_output(wait=True)\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.plot(m_opt_list)\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.plot(m_rand_list)\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(av_rewards)\n",
    "            plt.show()\n",
    "        num_trained_games += 1\n",
    "        env.reset()\n",
    "    m_opt_per_epsilon[epsilon_opt] = m_opt_list\n",
    "    m_rand_per_epsilon[epsilon_opt] = m_rand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './runs/Q4/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "with open(os.path.join(log_dir, 'm_opt_per_epsilon.pkl'), 'wb+') as f:\n",
    "    pickle.dump(m_opt_per_epsilon, f)\n",
    "\n",
    "with open(os.path.join(log_dir, 'm_opt_per_epsilon.pkl'), 'wb+') as f:\n",
    "    pickle.dump(m_opt_per_epsilon, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-paper')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for e, m_opt_list in sorted(m_opt_per_epsilon.items()):\n",
    "    plt.plot(np.arange(0, num_trained_games, 250), m_opt_list, \"-\", label=\"$\\epsilon_{opt}$\"+f\"={e}\")\n",
    "plt.title(\"$M_{opt}$\" + f\" for every 250 games during training\\nLearning rate {alpha:.2f}, Discount factor: {gamma:.2f}, $n^*$: {n_star}, \"+\"$\\epsilon_{min}$: \"+str(epsilon_min)+\" $\\epsilon_{max}$: \"+str(epsilon_max))\n",
    "plt.ylabel(\"$M_{opt}$ for every 250 games\")\n",
    "plt.xlabel(\"Number of training games\")\n",
    "plt.legend()\n",
    "plt.grid(color=\"lightgrey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-paper')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for e, m_rand_list in sorted(m_rand_per_epsilon.items()):\n",
    "    plt.plot(np.arange(0, num_trained_games, 250), m_rand_list, \"-\", label=\"$\\epsilon_{opt}$\"+f\"={e}\")\n",
    "plt.title(\"$M_{rand}$\" + f\" for every 250 games during training\\nLearning rate {alpha:.2f}, Discount factor: {gamma:.2f}, $n^*$: {n_star}, \"+\"$\\epsilon_{min}$: \"+str(epsilon_min)+\" $\\epsilon_{max}$: \"+str(epsilon_max))\n",
    "plt.ylabel(\"$M_{rand}$ for every 250 games\")\n",
    "plt.xlabel(\"Number of training games\")\n",
    "plt.legend()\n",
    "plt.grid(color=\"lightgrey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_m_opt = max(max(l) for l in m_opt_per_epsilon.values())\n",
    "max_m_rand = max(max(l) for l in m_rand_per_epsilon.values())\n",
    "max_m_opt, max_m_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Learning by self-practice\n",
    "\n",
    "In this section, your are supposed to ask whether $Q$-learning can learn to play Tic Tac Toe by only playing against itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different values of $\\epsilon \\in[0,1)$, run a $Q$-learning agent against itself for 20'000 games - i.e. both players use the same set of $Q$-values and update the same set of $Q$-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 # learning rate \n",
    "gamma = 0.99 # discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_opt_against_itself = {}\n",
    "m_rand_against_itself = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_star = 10000\n",
    "num_games = 20000\n",
    "\n",
    "for epsilon in (0, 0.25, 0.50, 0.75):\n",
    "    m_opt_list = []\n",
    "    m_rand_list = []\n",
    "    Q_table = QTable()\n",
    "    policy = EpsilonGreedy(Q_table, epsilon=epsilon)\n",
    "    env = TictactoeEnv()\n",
    "    \n",
    "    num_trained_games = 0\n",
    "    rewards_players1 = []\n",
    "    avg_rewards_players1 = []\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for itr in range(num_games):\n",
    "        env.reset()\n",
    "        state0, end, _ = env.observe()\n",
    "\n",
    "        current_player = 'X'\n",
    "        next_player = 'O'\n",
    "\n",
    "        move1 = policy.act(state0)\n",
    "        state1, end, _ = env.step(move1)\n",
    "\n",
    "        while not end:\n",
    "            current_player, next_player = next_player, current_player\n",
    "\n",
    "            move2 = policy.act(state1)\n",
    "            state2, end, _ = env.step(move2)\n",
    "            reward = env.reward(player = next_player)\n",
    "\n",
    "            Q_table[state0][move1] += alpha * (reward + gamma * max(Q_table[state2].values()) - Q_table[state0][move1])\n",
    "\n",
    "            state0 = state1.copy()\n",
    "            state1 = state2.copy()\n",
    "            move1 = move2\n",
    "\n",
    "        reward = env.reward(player = current_player)\n",
    "        Q_table[state0][move1] += alpha * (reward - Q_table[state0][move1])\n",
    "        \n",
    "\n",
    "        rewards_players1.append(env.reward(player = 'X'))\n",
    "        if len(rewards_players1) >= 250:\n",
    "            policy.set_epsilon(0)\n",
    "            m_opt_list.append(M_opt(policy))\n",
    "            m_rand_list.append(M_rand(policy))\n",
    "            policy.set_epsilon(epsilon)\n",
    "            avg_rewards_players1.append(np.mean(rewards_players1))\n",
    "            rewards_players1 = []\n",
    "            clear_output(wait=True)\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.plot(m_opt_list)\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.plot(m_rand_list)\n",
    "            plt.show()\n",
    "\n",
    "        num_trained_games += 1\n",
    "        env.reset()\n",
    "    m_opt_against_itself[epsilon] = m_opt_list\n",
    "    m_rand_against_itself[epsilon] = m_rand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './runs/Q7/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "with open(os.path.join(log_dir, 'm_opt_against_itself.pkl'), 'wb+') as f:\n",
    "    pickle.dump(m_opt_against_itself, f)\n",
    "\n",
    "with open(os.path.join(log_dir, 'm_rand_against_itself.pkl'), 'wb+') as f:\n",
    "    pickle.dump(m_rand_against_itself, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-paper')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for e, m_opt_list in sorted(m_opt_against_itself.items()):\n",
    "    plt.plot(np.arange(0, num_trained_games, 250), m_opt_list, \"-\", label=\"$\\epsilon$\"+f\"={e}\")\n",
    "plt.title(\"$M_{opt}$\" + f\" for every 250 games during training\\nLearning rate {alpha:.2f}, Discount factor: {gamma:.2f}\")\n",
    "plt.ylabel(\"$M_{opt}$ for every 250 games\")\n",
    "plt.xlabel(\"Number of training games\")\n",
    "plt.legend()\n",
    "plt.grid(color=\"lightgrey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-paper')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for e, m_rand_list in sorted(m_rand_against_itself.items()):\n",
    "    plt.plot(np.arange(0, num_trained_games, 250), m_rand_list, \"-\", label=\"$\\epsilon$\"+f\"={e}\")\n",
    "plt.title(\"$M_{rand}$\" + f\" for every 250 games during training\\nLearning rate {alpha:.2f}, Discount factor: {gamma:.2f}\")\n",
    "plt.ylabel(\"$M_{rand}$ for every 250 games\")\n",
    "plt.xlabel(\"Number of training games\")\n",
    "plt.legend()\n",
    "plt.grid(color=\"lightgrey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(min(v.values()) for v in Q_table.values()), max(max(v.values()) for v in Q_table.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rest of this section, use $\\epsilon(n)$ in Equation 1 with different values of $n^*$ instead of fixing  $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 # learning rate \n",
    "gamma = 0.99 # discount factor\n",
    "epsilon_min, epsilon_max = 0.1, 0.8 # exploration rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_rewards_dict = {}\n",
    "m_opt_dict = {}\n",
    "m_rand_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 20000\n",
    "\n",
    "for n_star in (1, 5_000, 10_000, 20_000, 40_000):\n",
    "    Q_table = QTable()\n",
    "    policy = EpsilonGreedyDecreasingExploration(Q_table, \n",
    "                                                epsilon_min=epsilon_min, \n",
    "                                                epsilon_max=epsilon_max, \n",
    "                                                n_star=n_star)\n",
    "    env = TictactoeEnv()\n",
    "    \n",
    "    rewards_players1 = []\n",
    "    avg_rewards_players1 = []\n",
    "    m_opt_list = []\n",
    "    m_rand_list = []\n",
    "    num_trained_games = 0\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for itr in range(num_games):\n",
    "        env.reset()\n",
    "        state0, end, _ = env.observe()\n",
    "\n",
    "        current_player = 'X'\n",
    "        next_player = 'O'\n",
    "        \n",
    "        policy.update_epsilon(itr)\n",
    "        move1 = policy.act(state0)\n",
    "        state1, end, _ = env.step(move1)\n",
    "\n",
    "        while not end:\n",
    "            current_player, next_player = next_player, current_player\n",
    "\n",
    "            move2 = policy.act(state1)\n",
    "            state2, end, _ = env.step(move2)\n",
    "            reward = env.reward(player = next_player)\n",
    "\n",
    "            Q_table[state0][move1] += alpha * (reward + gamma * max(Q_table[state2].values()) - Q_table[state0][move1])\n",
    "\n",
    "            state0 = state1.copy()\n",
    "            state1 = state2.copy()\n",
    "            move1 = move2\n",
    "\n",
    "        reward = env.reward(player = current_player)\n",
    "        Q_table[state0][move1] += alpha * (reward - Q_table[state0][move1])\n",
    "        \n",
    "\n",
    "        rewards_players1.append(env.reward(player = 'X'))\n",
    "        if len(rewards_players1) >= 250:\n",
    "            policy.set_epsilon(0)\n",
    "            m_opt_list.append(M_opt(policy))\n",
    "            m_rand_list.append(M_rand(policy))\n",
    "            avg_rewards_players1.append(np.mean(rewards_players1))\n",
    "            policy.update_epsilon(itr)\n",
    "            rewards_players1 = []\n",
    "            clear_output(wait=True)\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.plot(m_opt_list)\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.plot(m_rand_list)\n",
    "            plt.show()\n",
    "        num_trained_games += 1\n",
    "        env.reset()\n",
    "    av_rewards_dict[n_star] = avg_rewards_players1\n",
    "    m_opt_dict[n_star] = m_opt_list\n",
    "    m_rand_dict[n_star] = m_rand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './runs/Q8/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "with open(os.path.join(log_dir, 'm_opt_dict.pkl'), 'wb+') as f:\n",
    "    pickle.dump(m_opt_dict, f)\n",
    "\n",
    "with open(os.path.join(log_dir, 'm_rand_dict.pkl'), 'wb+') as f:\n",
    "    pickle.dump(m_rand_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-paper')\n",
    "plt.figure(figsize=(10, 5))\n",
    "for n_star, m_opt_list in sorted(m_opt_dict.items()):\n",
    "    plt.plot(np.arange(0, num_trained_games, 250), m_opt_list, \"-\", label=f\"$n*={n_star}$\")\n",
    "plt.title(\"$M_{opt}$\" + f\" for every 250 games during training\\nLearning rate {alpha:.2f}, Discount factor: {gamma:.2f},\"+\" $\\epsilon_{min}$: \"+str(epsilon_min)+\" $\\epsilon_{max}$: \"+str(epsilon_max))\n",
    "plt.ylabel(\"$M_{opt}$ for every 250 games\")\n",
    "plt.xlabel(\"Number of training games\")\n",
    "plt.legend()\n",
    "plt.grid(color=\"lightgrey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.style.use('seaborn-paper')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for n_star, m_rand_list in sorted(m_rand_dict.items()):\n",
    "    plt.plot(np.arange(0, num_trained_games, 250), m_rand_list, \"-\", label=f\"$n*={n_star}$\")\n",
    "plt.title(\"$M_{rand}$\" + f\" for every 250 games during training\\nLearning rate {alpha:.2f}, Discount factor: {gamma:.2f},\"+\" $\\epsilon_{min}$: \"+str(epsilon_min)+\" $\\epsilon_{max}$: \"+str(epsilon_max))\n",
    "plt.ylabel(\"$M_{rand}$ for every 250 games\")\n",
    "plt.xlabel(\"Number of training games\")\n",
    "plt.legend()\n",
    "plt.grid(color=\"lightgrey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all Q-values as images and save model for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value2player = {0: '-', 1: 'X', -1: 'O'}\n",
    "for i, s_tuple in enumerate(list(Q_table.keys())):\n",
    "    try:\n",
    "        plt.close()\n",
    "        name = \"\".join(value2player[v] for v in s_tuple)\n",
    "        s = np.array(s_tuple).reshape((3, 3))\n",
    "        q = np.empty((3, 3))\n",
    "        q[:] = np.NaN\n",
    "        for coord, value in Q_table[s].items():\n",
    "            q[coord] = value\n",
    "        plot_grid(s, q)\n",
    "        plt.clim(-1,1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"images/debug_Q_table/{name}.png\")\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "    except AttributeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/Q_table_15.27.pkl', 'wb') as f:\n",
    "    pickle.dump(Q_table, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'models/Q_table_Q8_04.05.2022_22.15.pkl', 'rb') as f:\n",
    "    Q_table = pickle.load(f)\n",
    "Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = (\n",
    "    ((0, 0, 0, 0, 0, 0, 0, 0, 0), 1),\n",
    "    ((0, -1, -1, 0, 1, 1, 0, 0, 0), 1),\n",
    "    ((0, -1, -1, 0, 1, 1, 1, 0, 0), -1),   \n",
    ")\n",
    "\n",
    "for i, (state, player) in enumerate(states):\n",
    "    plt.close()\n",
    "    s = np.array(state).reshape((3, 3))\n",
    "    q = np.empty((3, 3))\n",
    "    q[:] = np.NaN\n",
    "    for coord, value in Q_table[s].items():\n",
    "        q[coord] = value\n",
    "    plot_grid(s, q, clim=(-1,1))\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "372ce911d423ca6e068b96540316b9fc345100121f19e601a404720b795d2544"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ai-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
